{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03697c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='tokenizer/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '!'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tokenizer/gpt2')\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733f6090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['question', 'answer'],\n",
       "         num_rows: 71745\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['question', 'answer'],\n",
       "         num_rows: 200\n",
       "     })\n",
       " }),\n",
       " {'question': [22866,\n",
       "   25,\n",
       "   43387,\n",
       "   6158,\n",
       "   43679,\n",
       "   3084,\n",
       "   62,\n",
       "   3672,\n",
       "   62,\n",
       "   2414,\n",
       "   357,\n",
       "   354,\n",
       "   20297,\n",
       "   569,\n",
       "   31315,\n",
       "   1503,\n",
       "   11,\n",
       "   614,\n",
       "   17828,\n",
       "   7156,\n",
       "   1137,\n",
       "   8,\n",
       "   1808,\n",
       "   25,\n",
       "   2061,\n",
       "   318,\n",
       "   262,\n",
       "   24587,\n",
       "   706,\n",
       "   10249,\n",
       "   30,\n",
       "   3280,\n",
       "   25],\n",
       "  'answer': [46506,\n",
       "   24587,\n",
       "   16034,\n",
       "   3084,\n",
       "   62,\n",
       "   3672,\n",
       "   62,\n",
       "   2414,\n",
       "   33411,\n",
       "   614,\n",
       "   1875,\n",
       "   10249]})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('dataset/b-mc2/sql-create-context')['train']\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    question = 'context:%s question:%s answer:' % (data['context'],\n",
    "                                                   data['question'])\n",
    "    answer = data['answer']\n",
    "\n",
    "    question = tokenizer.encode(question, add_special_tokens=False)\n",
    "    answer = tokenizer.encode(answer, add_special_tokens=False)\n",
    "\n",
    "    return {'question': question, 'answer': answer}\n",
    "\n",
    "\n",
    "dataset = dataset.map(f, remove_columns=['context'])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    question = len(data['question'])\n",
    "    answer = len(data['answer'])\n",
    "    return 25 <= question <= 65 and 10 <= answer <= 35\n",
    "\n",
    "\n",
    "dataset = dataset.filter(f)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=200)\n",
    "\n",
    "dataset, dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e940d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       "  'label': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          ...,\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')},\n",
       " {'input_ids': tensor([[22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0],\n",
       "          [22866,    25, 43387,  ...,     0,     0,     0]], device='cuda:0'),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       "  'label': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          ...,\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100],\n",
       "          [-100, -100, -100,  ..., -100, -100, -100]], device='cuda:0')})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch_data():\n",
    "\n",
    "    def pad(data, split, lens):\n",
    "        #做个白板\n",
    "        input_ids = torch.full((len(data), lens),\n",
    "                               tokenizer.pad_token_id,\n",
    "                               device=device)\n",
    "\n",
    "        #往白板里黏贴数据\n",
    "        for i, d in enumerate(data):\n",
    "            input_ids[i, :len(d)] = torch.LongTensor(d)\n",
    "\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "        #计算label\n",
    "        label = input_ids.clone()\n",
    "        for l, s in zip(label, split):\n",
    "            #问题和pad的位置是-100\n",
    "            l[:s] = -100\n",
    "            l[l == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "    sample = random.choices(dataset['train'], k=16)\n",
    "    question = [i['question'] for i in sample]\n",
    "    answer = [i['answer'] for i in sample]\n",
    "    split = [len(i) for i in question]\n",
    "\n",
    "    #正确的问答\n",
    "    choice = [\n",
    "        q + a + [tokenizer.eos_token_id] for q, a in zip(question, answer)\n",
    "    ]\n",
    "\n",
    "    #错误的回答简单地定义为空回答就可以了\n",
    "    reject = [q + [tokenizer.eos_token_id] for q, a in zip(question, answer)]\n",
    "\n",
    "    #求最大长度\n",
    "    lens = max([len(i) for i in choice])\n",
    "\n",
    "    return pad(choice, split, lens), pad(reject, split, lens)\n",
    "\n",
    "\n",
    "get_batch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c47306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDPO(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        from transformers import AutoModelForCausalLM\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained('model/gpt2')\n",
    "\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        return self.model.lm_head(out)\n",
    "\n",
    "\n",
    "model_dpo = ModelDPO()\n",
    "model_dpo_ref = ModelDPO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e48bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context:CREATE TABLE table_name_8 (pos VARCHAR, date_from VARCHAR) question:What is Pos., when Date From is \"28 August 2008\"? answer:: \"What is, when is a table?\"\\n\\nThe table_name_name__name_8 is a string that contains the name of the table name.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(input_ids):\n",
    "    lens = input_ids.shape[1]\n",
    "    while True:\n",
    "        out = model_dpo(input_ids=input_ids,\n",
    "                        attention_mask=torch.ones_like(input_ids))\n",
    "        topk = out[0, -1].topk(1)\n",
    "\n",
    "        values = topk.values.softmax(0).tolist()\n",
    "        indices = topk.indices.tolist()\n",
    "        next_word = random.choices(indices, weights=values)\n",
    "\n",
    "        next_word = torch.LongTensor(next_word).unsqueeze(0).to('cuda')\n",
    "        input_ids = torch.cat([input_ids, next_word], dim=1)\n",
    "\n",
    "        if input_ids.shape[1] - lens >= 35:\n",
    "            break\n",
    "\n",
    "        if input_ids[0, -1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "input_ids = dataset['test'][0]['question']\n",
    "input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "\n",
    "out = generate(input_ids)\n",
    "\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c35e3b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-58.1640, -61.3286, -39.4775, -71.7320, -62.9812, -67.9002, -35.7873,\n",
       "        -55.2316, -55.5689, -73.9519, -81.4958, -37.8672, -55.4771, -86.0505,\n",
       "        -48.4158, -43.4941], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prob_log(model, choice, reject):\n",
    "    b = choice['input_ids'].shape[0]\n",
    "\n",
    "    #合并两部分输入,同时计算以提高效率\n",
    "    #[b, 21]\n",
    "    input_ids = torch.cat([choice['input_ids'], reject['input_ids']], dim=0)\n",
    "    attention_mask = torch.cat(\n",
    "        [choice['attention_mask'], reject['attention_mask']], dim=0)\n",
    "    label = torch.cat([choice['label'], reject['label']], dim=0)\n",
    "\n",
    "    #[b, 21, 28]\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    #偏移以对齐\n",
    "    #[b, 20]\n",
    "    label = label[:, 1:]\n",
    "    #[b, 20, 28]\n",
    "    out = out[:, :-1]\n",
    "\n",
    "    #取所有字的预测概率,因为要求联合概率,所以取对数\n",
    "    out = (out.softmax(2) + 1e-8).log()\n",
    "\n",
    "    #取预测到label的概率\n",
    "    #索引不能是负数,所以这里把负数置0\n",
    "    index = label.clone().unsqueeze(2)\n",
    "    index[index == -100] = 0\n",
    "    prob = out.gather(2, index=index).squeeze(2)\n",
    "\n",
    "    #只取答案部分的loss,筛选后,所有答案的概率对数求和\n",
    "    prob = (prob * (label != -100)).sum(1)\n",
    "\n",
    "    #choice和reject的预测概率求差\n",
    "    return prob[:b] - prob[b:]\n",
    "\n",
    "\n",
    "get_prob_log(model_dpo, *get_batch_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb049323",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 context:CREATE TABLE table_name_50 (character_name VARCHAR, voice_actor__english_1998___pioneer_ VARCHAR) question:what character did Laara sadiq play answer:what character did Laara play answer:what character did Laara sadiq play answer:what character did Laara sadiq play answer:what character did Laara play answer\n",
      "=========\n",
      "SELECT character_name FROM table_name_50 WHERE voice_actor__english_1998___pioneer_ = \"laara sadiq\"\n",
      "=========\n",
      "100 context:CREATE TABLE table_name_94 (position VARCHAR, pick VARCHAR) question:What is pick 246's position? answer:SELECT position FROM table_name_94 WHERE position = \"pick 246\"<|endoftext|>\n",
      "=========\n",
      "SELECT position FROM table_name_94 WHERE pick = 246\n",
      "=========\n",
      "200 context:CREATE TABLE table_name_78 (division_record VARCHAR, school VARCHAR) question:What is the division record for Woodbridge? answer:SELECT division_record FROM table_name_78 WHERE school = \"woodbridge\"<|endoftext|>\n",
      "=========\n",
      "SELECT division_record FROM table_name_78 WHERE school = \"woodbridge\"\n",
      "=========\n",
      "300 context:CREATE TABLE table_name_20 (name VARCHAR, nat VARCHAR) question:What's the name of MKD? answer:SELECT name FROM table_name_20 WHERE nat = \"MKD\"<|endoftext|>\n",
      "=========\n",
      "SELECT name FROM table_name_20 WHERE nat = \"mkd\"\n",
      "=========\n",
      "400 context:CREATE TABLE table_21313327_1 (written_by VARCHAR, no_in_season VARCHAR) question:In season number 3,  who were the writers? answer:SELECT written_by FROM table_21313327_1 WHERE no_in_season = 3<|endoftext|>\n",
      "=========\n",
      "SELECT written_by FROM table_21313327_1 WHERE no_in_season = 3\n",
      "=========\n",
      "500 context:CREATE TABLE table_name_95 (grid INTEGER, time_retired VARCHAR, laps VARCHAR) question:what is the highest grid when the time/retired is fuel pump and the laps is more than 26? answer:SELECT MAX(grid) FROM table_name_95 WHERE time_retired = \"pump and laps\" AND laps > 26<|endoftext|>\n",
      "=========\n",
      "SELECT MAX(grid) FROM table_name_95 WHERE time_retired = \"fuel pump\" AND laps > 26\n",
      "=========\n",
      "600 context:CREATE TABLE table_name_31 (top_10 INTEGER, top_25 VARCHAR, cuts_made VARCHAR) question:What is the average number of top-10s for the major with 2 top-25s and fewer than 10 cuts made? answer:SELECT AVG(top_10) FROM table_name_31 WHERE cuts_made = 2 AND top_25 < 10<|endoftext|>\n",
      "=========\n",
      "SELECT AVG(top_10) FROM table_name_31 WHERE top_25 = 2 AND cuts_made < 10\n",
      "=========\n",
      "700 context:CREATE TABLE table_name_94 (position VARCHAR, pick VARCHAR) question:What is pick 246's position? answer:SELECT position FROM table_name_94 WHERE pick = \"246\"<|endoftext|>\n",
      "=========\n",
      "SELECT position FROM table_name_94 WHERE pick = 246\n",
      "=========\n",
      "800 context:CREATE TABLE table_17358515_1 (points_2 VARCHAR, team VARCHAR) question:How many points did Goole Town accumulate? answer:SELECT points_2 FROM table_17358515_1 WHERE team = \"Goole Town\"<|endoftext|>\n",
      "=========\n",
      "SELECT COUNT(points_2) FROM table_17358515_1 WHERE team = \"Goole Town\"\n",
      "=========\n",
      "900 context:CREATE TABLE table_11677691_9 (hometown VARCHAR, college VARCHAR) question:Which hometown has the college Louisiana State? answer:SELECT hometown FROM table_11677691_9 WHERE college = \"Louisiana State\"<|endoftext|>\n",
      "=========\n",
      "SELECT hometown FROM table_11677691_9 WHERE college = \"Louisiana State\"\n",
      "=========\n",
      "1000 context:CREATE TABLE table_1341423_40 (candidates VARCHAR, incumbent VARCHAR) question:who are the candidates when the incumbent is lindsey graham? answer:SELECT candidates FROM table_1341423_40 WHERE incumbent = \"Lindsey Graham\"<|endoftext|>\n",
      "=========\n",
      "SELECT candidates FROM table_1341423_40 WHERE incumbent = \"Lindsey Graham\"\n",
      "=========\n",
      "1100 context:CREATE TABLE table_name_77 (country VARCHAR, series_premiere VARCHAR) question:Which country had a series that premiered on September 4, 2006? answer:SELECT country FROM table_name_77 WHERE series_premiere = \"september 4, 2006\"<|endoftext|>\n",
      "=========\n",
      "SELECT country FROM table_name_77 WHERE series_premiere = \"september 4, 2006\"\n",
      "=========\n",
      "1200 context:CREATE TABLE table_name_35 (director VARCHAR, title VARCHAR) question:Who is the director of Antz? answer:SELECT director FROM table_name_35 WHERE title = \"antz\"<|endoftext|>\n",
      "=========\n",
      "SELECT director FROM table_name_35 WHERE title = \"antz\"\n",
      "=========\n",
      "1300 context:CREATE TABLE table_name_55 (role VARCHAR, direction VARCHAR) question:Which role had thulasidas direction? answer:SELECT role FROM table_name_55 WHERE direction = \"thulasidas direction\"<|endoftext|>\n",
      "=========\n",
      "SELECT role FROM table_name_55 WHERE direction = \"thulasidas\"\n",
      "=========\n",
      "1400 context:CREATE TABLE table_24223834_3 (us_viewers__in_millions_ VARCHAR, directed_by VARCHAR) question:How many million U.S. viewers watched the episode that Daniel H. Forer directed? answer:SELECT COUNT(us_viewers__in_millions_) FROM table_24223834_3 WHERE directed_by = \"Daniel H. Forer\n",
      "=========\n",
      "SELECT us_viewers__in_millions_ FROM table_24223834_3 WHERE directed_by = \"Daniel H. Forer\"\n",
      "=========\n",
      "1500 context:CREATE TABLE table_name_89 (score VARCHAR, player VARCHAR) question:What is the score for Jock Hutchison? answer:SELECT score FROM table_name_89 WHERE player = \"jock hutch jock hutchison\"<|endoftext|>\n",
      "=========\n",
      "SELECT score FROM table_name_89 WHERE player = \"jock hutchison\"\n",
      "=========\n",
      "1600 context:CREATE TABLE table_24018430_3 (original_air_date VARCHAR, production_code VARCHAR) question:what is the original air date for production code 216? answer:SELECT original_air_date FROM table_24018430_3 WHERE production_code = \"216\"<|endoftext|>\n",
      "=========\n",
      "SELECT original_air_date FROM table_24018430_3 WHERE production_code = 216\n",
      "=========\n",
      "1700 context:CREATE TABLE table_name_32 (west VARCHAR, east VARCHAR) question:Who was in the West when ESV Gebensbach was in the East? answer:SELECT west FROM table_name_32 WHERE east = \"ebvesbach\"<|endoftext|>\n",
      "=========\n",
      "SELECT west FROM table_name_32 WHERE east = \"esv gebensbach\"\n",
      "=========\n",
      "1800 context:CREATE TABLE table_name_98 (to_par VARCHAR, margin_of_victory VARCHAR, tournament VARCHAR) question:What is To par, when Margin of Victory is \"2 Strokes\", and when Tournament is \"Women's British Open\"? answer:SELECT to_par FROM table_name_98 WHERE margin_of_victory = \"2 strokes\" AND tournament = \"women's British Open\"<|endoftext|>\n",
      "=========\n",
      "SELECT to_par FROM table_name_98 WHERE margin_of_victory = \"2 strokes\" AND tournament = \"women's british open\"\n",
      "=========\n",
      "1900 context:CREATE TABLE table_name_71 (wheels INTEGER, type VARCHAR, location VARCHAR) question:What average wheels has accounting as the type, with IBM Collection as the location? answer:SELECT AVG(wheels) FROM table_name_71 WHERE type = \"audit\" AND location = \"ibm collection\"<|endoftext|>\n",
      "=========\n",
      "SELECT AVG(wheels) FROM table_name_71 WHERE type = \"accounting\" AND location = \"ibm collection\"\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_dpo.parameters(),\n",
    "                             lr=1e-5,\n",
    "                             betas=(0.9, 0.999),\n",
    "                             eps=1e-8)\n",
    "\n",
    "for i in range(2000):\n",
    "    choice, reject = get_batch_data()\n",
    "\n",
    "    #两个模型分别计算概率对数\n",
    "    prob_log = get_prob_log(model_dpo, choice, reject)\n",
    "    with torch.no_grad():\n",
    "        prob_log_ref = get_prob_log(model_dpo_ref, choice, reject)\n",
    "\n",
    "    #两份概率计算kl散度\n",
    "    kl = -0.1 * (prob_log - prob_log_ref)\n",
    "\n",
    "    #以kl散度计算loss\n",
    "    loss = (kl.sigmoid() + 1e-8).log().mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        data = random.choice(dataset['test'])\n",
    "        input_ids = torch.LongTensor(data['question']).unsqueeze(0).to(device)\n",
    "\n",
    "        out = generate(input_ids)\n",
    "\n",
    "        print(i, tokenizer.decode(out[0]))\n",
    "        print('=========')\n",
    "        print(tokenizer.decode(data['answer']))\n",
    "        print('=========')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt2]",
   "language": "python",
   "name": "conda-env-pt2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
