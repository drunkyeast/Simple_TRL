{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab835cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='tokenizer/lvwerra/gpt2-imdb', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '!', 'additional_special_tokens': ['<|endoftext|>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tokenizer/lvwerra/gpt2-imdb')\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc57406a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question'],\n",
       "     num_rows: 50000\n",
       " }),\n",
       " {'question': [40, 26399, 314, 3001, 327]})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "dataset = load_from_disk('dataset/imdb')\n",
    "dataset = concatenate_datasets([dataset[i] for i in ['train', 'test']])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    question = tokenizer.encode(data['text'], add_special_tokens=False)[:5]\n",
    "    return {'question': question}\n",
    "\n",
    "\n",
    "dataset = dataset.map(f, remove_columns=['label', 'text'])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    return len(data['question']) == 5\n",
    "\n",
    "\n",
    "dataset = dataset.filter(f)\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12dbc381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0],\n",
       " [[16, 40, 423, 1775, 257, 1256],\n",
       "  [16, 33, 18058, 286, 262, 4044],\n",
       "  [15, 1, 464, 13037, 6289, 1],\n",
       "  [15, 40, 460, 470, 3505, 618],\n",
       "  [16, 1212, 318, 262, 5290, 286],\n",
       "  [15, 40, 1816, 284, 766, 428],\n",
       "  [15, 6109, 530, 815, 766, 428],\n",
       "  [16, 1, 36091, 6592, 6711, 1],\n",
       "  [15, 35700, 922, 2646, 422, 3771],\n",
       "  [15, 36, 292, 813, 262, 5290],\n",
       "  [15, 5211, 407, 766, 428, 2646],\n",
       "  [15, 2504, 338, 257, 2089, 11],\n",
       "  [16, 2061, 257, 6283, 8137, 318],\n",
       "  [16, 33, 463, 16988, 290, 4768],\n",
       "  [16, 35596, 1234, 11, 428, 318],\n",
       "  [15, 3792, 340, 655, 502, 11],\n",
       "  [15, 1212, 428, 2406, 286, 2479],\n",
       "  [15, 40, 550, 878, 257, 4203],\n",
       "  [15, 40, 4240, 644, 15579, 286],\n",
       "  [16, 39, 1794, 699, 422, 923],\n",
       "  [16, 464, 1772, 286, 6409, 16122],\n",
       "  [16, 28211, 531, 326, 12887, 4462],\n",
       "  [16, 1212, 3807, 14071, 517, 621],\n",
       "  [15, 1532, 428, 2646, 4329, 257],\n",
       "  [15, 32, 5249, 9298, 11, 4713],\n",
       "  [16, 2061, 1312, 5465, 749, 287],\n",
       "  [16, 1, 22073, 588, 257, 47808],\n",
       "  [15, 40, 10014, 4379, 428, 2646],\n",
       "  [16, 3351, 533, 47114, 318, 900],\n",
       "  [15, 40, 2492, 470, 12451, 284],\n",
       "  [15, 13295, 8849, 329, 6319, 2879],\n",
       "  [16, 1212, 4141, 2646, 318, 13519],\n",
       "  [15, 18050, 3944, 17731, 318, 1719],\n",
       "  [16, 18690, 1312, 760, 262, 2656],\n",
       "  [15, 1, 1639, 460, 7866, 1997],\n",
       "  [15, 464, 734, 1243, 389, 389],\n",
       "  [15, 25596, 948, 31078, 18258, 290],\n",
       "  [15, 6, 32, 17388, 286, 4930],\n",
       "  [15, 1212, 3807, 318, 2279, 475],\n",
       "  [15, 1, 17821, 1, 1621, 286],\n",
       "  [16, 464, 1578, 1829, 3170, 262],\n",
       "  [16, 83, 359, 18804, 2540, 302],\n",
       "  [15, 40, 655, 5201, 4964, 428],\n",
       "  [16, 2348, 292, 11, 3595, 4345],\n",
       "  [15, 1212, 6918, 318, 262, 1266],\n",
       "  [15, 5189, 262, 1115, 816, 1124],\n",
       "  [16, 21448, 43693, 2879, 6918, 547],\n",
       "  [15, 4711, 8088, 326, 1624, 428],\n",
       "  [16, 1722, 584, 8088, 423, 5081],\n",
       "  [15, 40, 1043, 428, 281, 45421],\n",
       "  [16, 36837, 11, 314, 2630, 326],\n",
       "  [16, 1212, 3807, 318, 12659, 13],\n",
       "  [16, 40, 716, 257, 22197, 18854],\n",
       "  [16, 40, 1053, 34310, 383, 25976],\n",
       "  [16, 3260, 616, 718, 614, 1468],\n",
       "  [15, 25681, 1497, 422, 428, 3807],\n",
       "  [16, 40, 373, 18680, 12617, 351],\n",
       "  [15, 5195, 1422, 470, 41542, 423],\n",
       "  [16, 40, 561, 423, 8359, 428],\n",
       "  [16, 1212, 318, 262, 2081, 1621],\n",
       "  [16, 1212, 3807, 318, 44089, 355],\n",
       "  [16, 40, 717, 2497, 366, 29449],\n",
       "  [16, 11380, 11, 880, 11, 645],\n",
       "  [15, 40, 2497, 428, 2646, 355],\n",
       "  [16, 1212, 318, 546, 257, 8805],\n",
       "  [15, 15496, 661, 11, 27, 1671],\n",
       "  [16, 46, 57, 318, 281, 1468],\n",
       "  [15, 16454, 11, 314, 655, 550],\n",
       "  [15, 40, 1276, 910, 314, 373],\n",
       "  [15, 16773, 319, 11, 1309, 338],\n",
       "  [16, 40, 5257, 284, 1700, 3336],\n",
       "  [15, 1212, 3807, 1107, 2523, 663],\n",
       "  [15, 2215, 2077, 355, 257, 2187],\n",
       "  [15, 8332, 663, 2479, 11, 428],\n",
       "  [16, 3844, 314, 550, 262, 1266],\n",
       "  [16, 40, 550, 2938, 257, 6547],\n",
       "  [16, 2601, 7626, 36389, 6026, 344],\n",
       "  [15, 1212, 3807, 318, 7579, 11211],\n",
       "  [16, 464, 7157, 286, 5830, 22430],\n",
       "  [16, 1858, 423, 587, 1811, 7328],\n",
       "  [15, 1532, 345, 389, 3612, 286],\n",
       "  [16, 2514, 1998, 7123, 345, 1107],\n",
       "  [16, 40, 1807, 262, 3807, 366],\n",
       "  [16, 1858, 389, 617, 1243, 314],\n",
       "  [16, 42338, 11, 314, 836, 18265],\n",
       "  [16, 9, 31895, 6509, 9, 27],\n",
       "  [15, 464, 36947, 318, 281, 4998],\n",
       "  [15, 40, 5543, 1842, 428, 2646],\n",
       "  [15, 3666, 4004, 905, 286, 477],\n",
       "  [16, 3673, 2089, 13289, 13, 5338],\n",
       "  [15, 32, 845, 23056, 1621, 11],\n",
       "  [15, 40817, 11, 379, 717, 11],\n",
       "  [16, 464, 717, 4756, 3715, 326],\n",
       "  [15, 40, 460, 1833, 703, 41921],\n",
       "  [16, 3260, 5586, 832, 262, 4512],\n",
       "  [16, 16454, 986, 568, 1312, 1053],\n",
       "  [15, 40, 3221, 588, 10997, 6918],\n",
       "  [15, 1212, 318, 407, 257, 922],\n",
       "  [16, 22017, 11, 428, 318, 845],\n",
       "  [16, 16676, 262, 1438, 7939, 43503],\n",
       "  [16, 2061, 4325, 618, 2130, 468],\n",
       "  [16, 2396, 30248, 340, 2125, 470],\n",
       "  [16, 49, 27015, 7328, 1464, 423],\n",
       "  [16, 11158, 257, 3807, 810, 262],\n",
       "  [15, 45, 14651, 17064, 293, 33500],\n",
       "  [15, 40, 1053, 1100, 257, 1271],\n",
       "  [15, 1, 2437, 1675, 44927, 14213],\n",
       "  [15, 5189, 262, 3478, 10544, 508],\n",
       "  [15, 464, 3807, 373, 3105, 11],\n",
       "  [15, 40, 7342, 428, 3807, 257],\n",
       "  [15, 50, 592, 12382, 25, 366],\n",
       "  [16, 818, 1502, 284, 2883, 705],\n",
       "  [15, 1, 8491, 921, 287, 262],\n",
       "  [16, 18690, 11, 523, 340, 338],\n",
       "  [15, 5703, 1392, 428, 287, 262],\n",
       "  [16, 1, 3856, 322, 4754, 1],\n",
       "  [15, 1858, 389, 867, 3840, 314],\n",
       "  [16, 40, 3088, 284, 1577, 428],\n",
       "  [15, 986, 392, 7685, 1312, 836],\n",
       "  [15, 19703, 357, 23865, 4687, 88],\n",
       "  [16, 1639, 714, 3800, 257, 2196],\n",
       "  [15, 464, 2646, 318, 1912, 319],\n",
       "  [15, 32, 32611, 3499, 923, 11],\n",
       "  [15, 4561, 9437, 364, 287, 428],\n",
       "  [15, 1, 2061, 561, 345, 466],\n",
       "  [15, 72, 892, 428, 905, 318],\n",
       "  [16, 1212, 3807, 318, 546, 257],\n",
       "  [15, 1558, 14, 2999, 14, 2931]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch_data():\n",
    "    label = random.choices(range(2), k=128)\n",
    "    question = random.choices(dataset, k=128)\n",
    "    question = [i['question'] for i in question]\n",
    "\n",
    "    question = [[tokenizer.convert_tokens_to_ids(str(l))] + q\n",
    "                for l, q in zip(label, question)]\n",
    "\n",
    "    return label, question\n",
    "\n",
    "\n",
    "get_batch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ee37d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/lvwerra/gpt2-imdb were not used when initializing GPT2LMHeadModel: ['v_head.summary.weight', 'v_head.summary.bias']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at model/lvwerra/gpt2-imdb were not used when initializing GPT2LMHeadModel: ['v_head.summary.weight', 'v_head.summary.bias']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class ModelPPO(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        from transformers import AutoModelForCausalLM\n",
    "\n",
    "        self.model_gen = AutoModelForCausalLM.from_pretrained(\n",
    "            'model/lvwerra/gpt2-imdb')\n",
    "\n",
    "        self.v_head = torch.nn.Sequential(torch.nn.Dropout(0.1),\n",
    "                                          torch.nn.Linear(768, 1))\n",
    "\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_hidden_state = self.model_gen.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True).last_hidden_state\n",
    "\n",
    "        logits = self.model_gen.lm_head(last_hidden_state)\n",
    "        value = self.v_head(last_hidden_state).squeeze(-1)\n",
    "\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "model_ppo = ModelPPO()\n",
    "model_ppo_ref = ModelPPO()\n",
    "\n",
    "for i in model_ppo_ref.parameters():\n",
    "    i.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5f88e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5304, -0.1680, -0.8149, -0.8351, -1.1403, -0.5243,  1.0303, -0.1843,\n",
       "         0.0434,  0.9319, -0.2327,  0.5249,  0.8442, -0.8438,  0.8771])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_kl(a, b):\n",
    "    method = 'kl'\n",
    "\n",
    "    if method == 'kl':\n",
    "        return a - b\n",
    "\n",
    "    if method == 'abs':\n",
    "        return (a - b).abs()\n",
    "\n",
    "    if method == 'mse':\n",
    "        return (a - b).square() * 0.5\n",
    "\n",
    "    if method == 'full':\n",
    "        return torch.nn.functional.kl_div(a,\n",
    "                                          b,\n",
    "                                          log_target=True,\n",
    "                                          reduction='none')\n",
    "\n",
    "\n",
    "get_kl(torch.randn(15), torch.zeros(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d600b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pt2/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PPOTrainer at 0x7f04cdfc8190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl.core import clip_by_value, logprobs_from_logits, masked_mean, masked_whiten\n",
    "\n",
    "\n",
    "class PPOTrainer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.optimizer = torch.optim.Adam(model_ppo.parameters(), lr=1e-5)\n",
    "\n",
    "    def step(self, question, answer, reward):\n",
    "        with torch.no_grad():\n",
    "            #编码\n",
    "            token = [q.tolist() + a.tolist() for q, a in zip(question, answer)]\n",
    "            token = [{'input_ids': i} for i in token]\n",
    "            token = tokenizer.pad(token, return_tensors='pt').to(device)\n",
    "            input_ids = token.input_ids\n",
    "            attention_mask = token.attention_mask\n",
    "            del token\n",
    "\n",
    "            #question和answer不需要内容,只需要长度信息即可\n",
    "            lens_q = [len(i) for i in question]\n",
    "            lens_a = [len(i) for i in answer]\n",
    "            del question\n",
    "            del answer\n",
    "\n",
    "            #根据question计算answer的概率,并计算每个动作的分数\n",
    "            prob_log, value, mask = self.batched_forward_pass(\n",
    "                model_ppo, input_ids, attention_mask, lens_q, lens_a)\n",
    "\n",
    "            #使用ref模型计算概率,这是为了计算kl散度\n",
    "            prob_log_ref, _, _ = self.batched_forward_pass(\n",
    "                model_ppo_ref, input_ids, attention_mask, lens_q, lens_a)\n",
    "\n",
    "            #计算两份概率的kl散度,并融入reward\n",
    "            reward = self.compute_rewards(reward, prob_log, prob_log_ref, mask)\n",
    "\n",
    "            #计算delta和target,用于计算loss\n",
    "            value, delta, target = self.compute_advantages(value, reward, mask)\n",
    "\n",
    "        #每批数据循环N次模型\n",
    "        for _ in range(4):\n",
    "            #每次算一个数据\n",
    "            for i in range(len(input_ids)):\n",
    "                #重新计算概率和value\n",
    "                prob_log_new, value_new, _ = self.batched_forward_pass(\n",
    "                    model_ppo, input_ids[i].unsqueeze(0),\n",
    "                    attention_mask[i].unsqueeze(0), [lens_q[i]], [lens_a[i]])\n",
    "\n",
    "                #根据新旧概率求出变化率,进而求出loss\n",
    "                #根据target和value的差可以计算出另外一份loss\n",
    "                loss = self.get_loss(prob_log[i].unsqueeze(0),\n",
    "                                     value[i].unsqueeze(0), prob_log_new,\n",
    "                                     value_new, mask[i].unsqueeze(0),\n",
    "                                     delta[i].unsqueeze(0),\n",
    "                                     target[i].unsqueeze(0))\n",
    "\n",
    "                if not loss:\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(model_ppo.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "    def batched_forward_pass(self, model, input_ids, attention_mask, lens_q,\n",
    "                             lens_a):\n",
    "        logits, value = model(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask)\n",
    "\n",
    "        #取每个字的概率对数\n",
    "        prob_log = logprobs_from_logits(logits[:, :-1], input_ids[:, 1:])\n",
    "\n",
    "        #是预测结果并且不是PAD的位置是1\n",
    "        mask = torch.zeros_like(attention_mask)\n",
    "        mask[:, :-1] = attention_mask[:, 1:]\n",
    "        for i in range(len(input_ids)):\n",
    "            start = lens_q[i] - 1\n",
    "            end = start + lens_a[i]\n",
    "            mask[i, :start] = 0\n",
    "            mask[i, end:] = 0\n",
    "\n",
    "        #对最后一个字的预测没有意义,直接丢弃\n",
    "        value = value[:, :-1]\n",
    "        mask = mask[:, :-1]\n",
    "\n",
    "        return prob_log, value, mask\n",
    "\n",
    "    def compute_rewards(self, reward, prob_log, prob_log_ref, mask):\n",
    "        reward_kl = []\n",
    "\n",
    "        for i in range(len(reward)):\n",
    "            #求两份概率的kl散度\n",
    "            kl = get_kl(prob_log[i], prob_log_ref[i]) * -0.2\n",
    "\n",
    "            #把reward加在最后一个字的kl散度上\n",
    "            if (mask[i] == 0).all():\n",
    "                #print('all 0')\n",
    "                idx = 0\n",
    "            else:\n",
    "                idx = mask[i].nonzero()[-1].item()\n",
    "            kl[idx] += reward[i]\n",
    "\n",
    "            reward_kl.append(kl)\n",
    "\n",
    "        return torch.stack(reward_kl)\n",
    "\n",
    "    def compute_advantages(self, value, reward_kl, mask):\n",
    "        value = value * mask\n",
    "        reward_kl = reward_kl * mask\n",
    "\n",
    "        delta = []\n",
    "        lens = reward_kl.shape[1]\n",
    "\n",
    "        #从后往前遍历\n",
    "        for i in reversed(range(lens)):\n",
    "            #取下一时刻的value,如果已经是最后一个时刻,则value_next是0\n",
    "            #因为整个循环是从后往前,所以第0次是0,其他时刻取value\n",
    "            value_next = 0\n",
    "            if i < lens - 1:\n",
    "                value_next = value[:, i + 1]\n",
    "\n",
    "            #value = gamma*下一时刻的value + reward\n",
    "            #理论上相等,这里的差定义为delta,这里gamma是1,所以省略了\n",
    "            d = reward_kl[:, i] + value_next - value[:, i]\n",
    "\n",
    "            #取最后一个delta,如果还没有,则初始化为0\n",
    "            last_d = 0\n",
    "            if delta:\n",
    "                last_d = delta[-1]\n",
    "\n",
    "            #delta是从后往前传递的,这里的系数衡量了前后动作的因果关联性\n",
    "            delta.append(d + 0.95 * last_d)\n",
    "\n",
    "        #翻转顺序\n",
    "        delta = torch.stack(delta[::-1]).transpose(0, 1)\n",
    "\n",
    "        #定义target,它估计了理想的value值\n",
    "        target = delta + value\n",
    "        delta = masked_whiten(delta, mask)\n",
    "\n",
    "        return value, delta, target\n",
    "\n",
    "    def get_loss(self, prob_log, value, prob_log_new, value_new, mask, delta,\n",
    "                 target):\n",
    "\n",
    "        #对数概率,相除变相减,取exp后还原为商,即两个模型输出logits的变化率\n",
    "        ratio = (prob_log_new - prob_log).exp()\n",
    "\n",
    "        #如果变化率太过于剧烈,可能是发生了震荡,跳过\n",
    "        if masked_mean(ratio, mask).item() > 10:\n",
    "            #print('skip', masked_mean(ratio, mask).item())\n",
    "            return None\n",
    "\n",
    "        #先算两个value的loss,简单的算mse loss就可以了\n",
    "        loss_vf1 = (value_new - target)**2\n",
    "        #数值裁剪,很显然是为了缓解自举\n",
    "        loss_vf2 = clip_by_value(value_new, value - 0.2, value + 0.2)\n",
    "        loss_vf2 = (loss_vf2 - target)**2\n",
    "        #两份loss取大的,还是为了缓解自举\n",
    "        loss_vf = 0.5 * masked_mean(torch.max(loss_vf1, loss_vf2), mask)\n",
    "\n",
    "        #计算ppo loss\n",
    "        loss_surr1 = -delta * ratio\n",
    "        #数值裁剪,很显然是为了缓解自举\n",
    "        loss_surr2 = -delta * ratio.clamp(0.8, 1.2)\n",
    "        loss_surr = masked_mean(torch.max(loss_surr1, loss_surr2), mask)\n",
    "\n",
    "        return loss_surr + 0.1 * loss_vf\n",
    "\n",
    "\n",
    "trainer = PPOTrainer()\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "805234d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer_cls = AutoTokenizer.from_pretrained(\n",
    "    'tokenizer/lvwerra/distilbert-imdb')\n",
    "model_cls = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'model/lvwerra/distilbert-imdb')\n",
    "model_cls.to(device)\n",
    "\n",
    "for i in model_cls.parameters():\n",
    "    i.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e7a327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "         0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
       " [tensor([  15, 2215,  257, 3807,  588,  366], device='cuda:0'),\n",
       "  tensor([   15, 17439, 10884, 26066,   750,   523], device='cuda:0'),\n",
       "  tensor([  15, 1532,  345,  389, 2045,  329], device='cuda:0'),\n",
       "  tensor([   15,    40, 13770,  8359,   428,  2646], device='cuda:0'),\n",
       "  tensor([   16,  1722,   262, 13738,  8146, 17607], device='cuda:0'),\n",
       "  tensor([  15, 1212, 2646,  815,  423,  587], device='cuda:0'),\n",
       "  tensor([   16, 12298,    72,    47,  8404,   284], device='cuda:0'),\n",
       "  tensor([   16, 42322,   314,   561, 18595,   661], device='cuda:0'),\n",
       "  tensor([  15,   40,  373, 1936,  618,  262], device='cuda:0'),\n",
       "  tensor([  16, 3666, 3988, 6151,  428, 3807], device='cuda:0')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_question():\n",
    "    label, question = get_batch_data()\n",
    "    label = torch.LongTensor(label).to(device)\n",
    "\n",
    "    question = [torch.LongTensor(i).to(device) for i in question]\n",
    "\n",
    "    return label, question\n",
    "\n",
    "\n",
    "label, question = get_question()\n",
    "\n",
    "label, question[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77e7d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  464,  7193,   261,     1,   373,   287, 20550,    11,   612,   714,\n",
       "           307,  2187,  3923,   546, 28623,   262,  2933,    11,   290,   703,\n",
       "           339,  1392,  4978,   287,   262, 16479,   287,   262,  1903,   284,\n",
       "          3095, 11445], device='cuda:0'),\n",
       " tensor([ 7138,    13,   679,   318,  4084,   379,   257,   966,   287,   465,\n",
       "          1981,  3451,   810,   339,   338,   655,   407,   379,   477, 39072,\n",
       "           355,   281,  6802, 29847,  1671,  1220,  6927,  1671, 11037, 48393,\n",
       "          1279,  1671], device='cuda:0'),\n",
       " tensor([  262,  1388,  3435,   351,  3499, 19063,    11, 13300,  4306,  1100,\n",
       "          1088,   329,   257, 39679,  3074,   475,   314,  4719,   340,   611,\n",
       "           345,   765,   284,  5899, 20775,    13, 50256], device='cuda:0'),\n",
       " tensor([   13,  1081,  2582,   355, 17369,   465, 12702,  9546,   314,   373,\n",
       "         21638,   585,     0, 50256], device='cuda:0'),\n",
       " tensor([ 2613,   459,     0,  1318,   338,   530,  7650,   414,  7411,   477,\n",
       "          1115,   286,   262,  4476,   274,  4379,   262,   366, 32399,  1911,\n",
       "          5278,   306,   340,   338, 13699,  4379,  9935,   440,     6,  6187,\n",
       "          2304,   692], device='cuda:0'),\n",
       " tensor([ 1695,   319, 34245, 12490,   284,  2342,   691,   986, 37814,   314,\n",
       "          7342,   373,   327,    42,  8206,    11,   772,  6918,   314,  1392,\n",
       "           379,  1790,  4003,    13,  1675,   910,   326,   428,  2646,   468,\n",
       "           262,  2694], device='cuda:0'),\n",
       " tensor([  466,   257,  3155,   286,  1243,    13,   770,   561,   407,   466,\n",
       "           284,  1388, 16687,    13,   383,  3807,   857,   407,   423,   262,\n",
       "           466,   540,  3404,   326,  7328,   884,   355,   383, 21469,  9501,\n",
       "           466,    13], device='cuda:0'),\n",
       " tensor([  284, 27401,  1598,   286,   883, 15774,  2324,   290,  7124,  9073,\n",
       "            13,   632,   481,  1790,  1103,  6918,    11,  2529, 11970,  3404,\n",
       "          5645,   510,   852,  1716,   517,  4388,   706,   257,  1178,  1933,\n",
       "            11,   611], device='cuda:0'),\n",
       " tensor([ 1621,  2492,   470,  1598,    13,  1318,   338,   691,  1936,   393,\n",
       "          2237,  2431,  1364,   674, 10281,     6,  3160,    13,   632,  2952,\n",
       "          2627,  1598,   284,   502,   355,   880,   355,   584, 15618,  7912,\n",
       "           326,   351], device='cuda:0'),\n",
       " tensor([   13,   632,   338,  7818,   266,    14,   262,   366, 11718,   500,\n",
       "         15461,   465, 22701,     1,   290,   366,  1169,  4293,    12, 20676,\n",
       "          6944,    11,   475,   281,  4896,   286,   640,   290,  2568,   326,\n",
       "          1854,   466], device='cuda:0')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#包装类,用于生成\n",
    "def generate(input_ids):\n",
    "    return model_ppo.model_gen.generate(input_ids=input_ids,\n",
    "                                        min_length=-1,\n",
    "                                        top_k=0.0,\n",
    "                                        top_p=1.0,\n",
    "                                        do_sample=True,\n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        max_new_tokens=32,\n",
    "                                        eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "def get_answer(question):\n",
    "    #如果question的长度确定,这里可以转换成批运算\n",
    "    if True:\n",
    "        answer = generate(torch.stack(question))\n",
    "\n",
    "        answer_new = []\n",
    "        for i in answer:\n",
    "            if tokenizer.eos_token_id not in i:\n",
    "                answer_new.append(i.unsqueeze(0))\n",
    "                continue\n",
    "            split = i.tolist().index(tokenizer.eos_token_id) + 1\n",
    "            answer_new.append(i[:split].unsqueeze(0))\n",
    "        answer = answer_new\n",
    "    else:\n",
    "        answer = [generate(i.unsqueeze(0)) for i in question]\n",
    "\n",
    "    #裁剪,只要生成的部分\n",
    "    answer = [a[0, len(q):] for q, a in zip(question, answer)]\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "answer = get_answer(question)\n",
    "\n",
    "answer[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "720b4c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1023, -2.0593,  0.1034, -2.1943, -0.2854,  0.3657, -2.0234, -0.6897,\n",
       "        -1.0600,  1.9237, -1.1278, -1.9625,  1.9247,  1.0706,  1.9642, -2.7512,\n",
       "         0.1869,  0.4816, -1.4677,  1.4251,  1.8278, -2.4232,  0.3808, -2.2452,\n",
       "        -1.4352, -1.3860,  1.2793,  2.5945,  0.8147,  1.0352, -0.8067,  1.5356,\n",
       "        -0.4795,  0.6381, -1.9992,  1.6725,  1.0273, -1.3380,  2.6569,  0.8692,\n",
       "         0.3785, -0.6434, -0.1132,  0.7105,  0.7786, -1.3073, -0.9017, -1.7745,\n",
       "        -1.1298, -0.0379,  0.9818, -1.9865, -1.9988, -2.0698, -2.3039,  0.5009,\n",
       "         0.7905, -1.5054, -0.3490, -1.1563,  0.6987,  1.6517, -1.2936, -1.1562,\n",
       "        -0.6478, -1.3625, -0.8715, -1.3332, -1.3316, -2.4711,  0.1419, -0.8977,\n",
       "         0.6781, -2.5032,  2.1797, -2.2050,  0.3033,  1.0208,  1.5972,  1.0242,\n",
       "         0.0768,  1.0985,  2.1549,  1.7992, -2.0123,  2.7790,  0.7879, -0.4145,\n",
       "         1.1283, -2.4656,  0.3194,  2.4502, -1.3679,  1.6667,  2.4101, -0.8367,\n",
       "         0.9427,  2.0877, -2.4355, -0.8031,  0.8273, -1.3980,  1.5957,  1.0456,\n",
       "         1.6853, -0.6226,  1.0414,  0.5680, -1.2993, -0.8639, -1.3040,  0.6334,\n",
       "         2.5615,  0.0733, -2.6728, -1.4309,  0.7316,  1.3707,  1.3495, -1.8754,\n",
       "         1.5368,  2.0362, -1.4510, -2.0597, -2.3088,  2.4779, -0.3622, -1.8313],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_reward(question, answer, label):\n",
    "    token = [q.tolist()[1:] + a.tolist() for q, a in zip(question, answer)]\n",
    "    token = [tokenizer.decode(i) for i in token]\n",
    "\n",
    "    token = tokenizer_cls(token,\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=512,\n",
    "                          return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_cls(**token).logits\n",
    "\n",
    "    return logits.gather(1, label.reshape(-1, 1)).squeeze(1)\n",
    "\n",
    "\n",
    "reward = get_reward(question, answer, label)\n",
    "\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1c70e8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.04424596205353737\n",
      "0Rowan Atkinson's Mr -> . Incredible not starting off flat or unless you were just seriously down to sea out here I did feel sorry because there is an absolutely amazing heist film thats almost -1.7402509450912476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.1893816739320755\n",
      "1Not only is this a ->  work of pure creativity. Its worth buying Justin Kuehne.<|endoftext|> 2.4963550567626953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.11868445575237274\n",
      "0I found this very touching -> .)<|endoftext|> -1.8980457782745361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0.04239548742771149\n",
      "0Even die hard John Wayne ->  was able to play that one scene apart by himself. <br /><br />The fight in the basement is a simple scene. That's the low- -0.8326716423034668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 0.09275282919406891\n",
      "0I couldn't wait to ->  find it on the prairie.<|endoftext|> -0.02781713753938675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.38358354568481445\n",
      "1The first episode set the ->  structure perfectly. It made me hate it.<|endoftext|> -0.7066333889961243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 1.2206898927688599\n",
      "1May 1938. Hitler in ->  this film is the best.<|endoftext|> 1.5236488580703735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 1.730372428894043\n",
      "1I disliked this film intensely ->  I wanted to finish it so often I didn't know what to do with the characters we never liked, but this film was amazing!<|endoftext|> 1.5757133960723877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 1.9782875776290894\n",
      "0This film is a very ->  bad framing.<|endoftext|> 2.4100003242492676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 2.1086392402648926\n",
      "1It is the early morning ->  in DC the best.<|endoftext|> 2.3762409687042236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 2.1353683471679688\n",
      "1It hurt to watch this -> , this, but it really felt ripping it into a piece and it makes it very well done.<|endoftext|> 2.6977579593658447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 2.251354455947876\n",
      "1This was the first of ->  such themes. It was great!<|endoftext|> 2.4420251846313477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 2.341010093688965\n",
      "1Everyone is surely familiar with ->  the name and is highly appreciated.<|endoftext|> 2.4222381114959717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 2.313877582550049\n",
      "1Whoever saddled this piece ->  is a great appreciated film.<|endoftext|> 2.3847947120666504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 2.2891697883605957\n",
      "1I can never fathom ->  this film blue shadows. I think it is wonderful.<|endoftext|> 2.648195743560791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 2.1408190727233887\n",
      "0Making a film for under ->  this crap!<|endoftext|> 2.4721293449401855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 2.212196111679077\n",
      "0Lets make a movie ->  about this crap.<|endoftext|> 2.4810938835144043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 2.2233328819274902\n",
      "0Just saw ICE AGE ->  for a total waste of time.<|endoftext|> 2.480029821395874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 2.2824764251708984\n",
      "0The apolitical musicians Eva ->  is just awful.<|endoftext|> 2.610349655151367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 2.3515257835388184\n",
      "1Excellent movie, a realistic ->  look of my life.<|endoftext|> 2.7795515060424805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    label, question = get_question()\n",
    "    answer = get_answer(question)\n",
    "    reward = get_reward(question, answer, label)\n",
    "\n",
    "    trainer.step(question, answer, reward)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch, reward.mean().item())\n",
    "        question = tokenizer.decode(question[0].tolist())\n",
    "        answer = tokenizer.decode(answer[0].tolist())\n",
    "        reward = reward[0].item()\n",
    "\n",
    "        #0差评,1好评\n",
    "        print(question, '->', answer, reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt2]",
   "language": "python",
   "name": "conda-env-pt2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
