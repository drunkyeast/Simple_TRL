{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab835cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pt2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='tokenizer/lvwerra/gpt2-imdb', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '!', 'additional_special_tokens': ['<|endoftext|>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tokenizer/lvwerra/gpt2-imdb')\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc57406a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question'],\n",
       "     num_rows: 50000\n",
       " }),\n",
       " {'question': [40, 26399, 314, 3001, 327]})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "dataset = load_from_disk('dataset/imdb')\n",
    "dataset = concatenate_datasets([dataset[i] for i in ['train', 'test']])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    question = tokenizer.encode(data['text'], add_special_tokens=False)[:5]\n",
    "    return {'question': question}\n",
    "\n",
    "\n",
    "dataset = dataset.map(f, remove_columns=['label', 'text'])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    return len(data['question']) == 5\n",
    "\n",
    "\n",
    "dataset = dataset.filter(f)\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12dbc381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0],\n",
       " [[16, 36623, 290, 15579, 1111, 2495],\n",
       "  [16, 39, 2743, 16543, 318, 546],\n",
       "  [16, 5962, 612, 373, 23459, 72],\n",
       "  [16, 7149, 2618, 508, 2925, 284],\n",
       "  [15, 40, 3505, 4964, 428, 319],\n",
       "  [16, 3152, 262, 7396, 11533, 286],\n",
       "  [15, 43920, 32520, 26494, 15992, 11],\n",
       "  [15, 27991, 1472, 318, 3737, 530],\n",
       "  [16, 4598, 407, 307, 6522, 28970],\n",
       "  [15, 464, 29274, 45270, 11, 7808],\n",
       "  [15, 1212, 2646, 2900, 510, 319],\n",
       "  [15, 464, 691, 661, 1312, 561],\n",
       "  [15, 2025, 21218, 306, 24007, 6833],\n",
       "  [15, 38202, 4835, 15300, 1525, 7504],\n",
       "  [15, 40, 550, 8359, 262, 15812],\n",
       "  [16, 4514, 2000, 286, 1450, 33743],\n",
       "  [16, 11006, 290, 24967, 7091, 7012],\n",
       "  [15, 5962, 314, 1276, 910, 326],\n",
       "  [15, 21448, 25699, 423, 4367, 262],\n",
       "  [16, 8499, 4379, 428, 2646, 329],\n",
       "  [16, 26556, 1468, 4947, 286, 3923],\n",
       "  [16, 5005, 7697, 360, 5945, 259],\n",
       "  [15, 3666, 477, 12, 2435, 4004],\n",
       "  [15, 13615, 284, 2185, 318, 257],\n",
       "  [16, 32, 21104, 12006, 290, 37928],\n",
       "  [15, 40, 423, 1239, 1100, 262],\n",
       "  [15, 1858, 318, 257, 1256, 2642],\n",
       "  [16, 1212, 3807, 373, 257, 13899],\n",
       "  [15, 1212, 318, 257, 6635, 7427],\n",
       "  [15, 40, 836, 470, 760, 703],\n",
       "  [15, 7, 4561, 9437, 364, 9426],\n",
       "  [16, 40, 460, 470, 1037, 475],\n",
       "  [16, 22442, 284, 428, 2646, 11],\n",
       "  [15, 40, 7342, 705, 4834, 7670],\n",
       "  [15, 1858, 318, 257, 1621, 357],\n",
       "  [15, 3198, 286, 2407, 257, 1178],\n",
       "  [16, 15784, 736, 625, 262, 1613],\n",
       "  [16, 2514, 307, 5508, 11, 262],\n",
       "  [15, 1212, 318, 655, 530, 286],\n",
       "  [15, 40, 1101, 407, 1654, 703],\n",
       "  [15, 1212, 3807, 318, 34445, 257],\n",
       "  [16, 7556, 2140, 2646, 546, 2646],\n",
       "  [16, 40, 1053, 1107, 8359, 428],\n",
       "  [16, 1, 818, 262, 995, 286],\n",
       "  [16, 17821, 3923, 787, 262, 1266],\n",
       "  [16, 1, 464, 7610, 5932, 1],\n",
       "  [15, 49738, 12754, 22732, 3807, 326],\n",
       "  [16, 464, 13172, 286, 2094, 11952],\n",
       "  [16, 5779, 11, 314, 3214, 329],\n",
       "  [16, 40, 1053, 1100, 617, 286],\n",
       "  [16, 19419, 257, 4719, 11, 530],\n",
       "  [16, 40, 716, 523, 9675, 44922],\n",
       "  [16, 3137, 7342, 340, 11, 502],\n",
       "  [16, 40, 1816, 284, 1524, 351],\n",
       "  [15, 1212, 905, 318, 1049, 13],\n",
       "  [16, 464, 717, 286, 1936, 520],\n",
       "  [16, 40, 423, 1239, 587, 355],\n",
       "  [16, 3237, 262, 6247, 7721, 276],\n",
       "  [16, 40, 3505, 4379, 262, 12268],\n",
       "  [16, 1212, 318, 257, 2646, 1312],\n",
       "  [15, 40, 9159, 314, 423, 257],\n",
       "  [15, 7975, 286, 262, 867, 7328],\n",
       "  [15, 21944, 273, 18365, 363, 11925],\n",
       "  [15, 1858, 338, 645, 779, 2111],\n",
       "  [16, 1212, 2646, 318, 2089, 13],\n",
       "  [15, 3844, 517, 621, 1683, 356],\n",
       "  [16, 11696, 13606, 11, 257, 6036],\n",
       "  [15, 32, 13206, 11, 5508, 11],\n",
       "  [16, 1026, 373, 28294, 0, 383],\n",
       "  [16, 5703, 44207, 1576, 284, 307],\n",
       "  [15, 40, 550, 30508, 262, 3807],\n",
       "  [15, 10814, 3730, 290, 4813, 0],\n",
       "  [16, 32, 6507, 11, 6507, 6504],\n",
       "  [15, 28718, 5285, 25, 383, 15875],\n",
       "  [15, 3792, 612, 257, 1492, 11946],\n",
       "  [15, 34, 49399, 1313, 373, 12132],\n",
       "  [16, 1639, 561, 1107, 761, 284],\n",
       "  [15, 4480, 644, 484, 550, 13],\n",
       "  [15, 1212, 2646, 3607, 649, 3616],\n",
       "  [16, 40, 550, 7342, 1811, 1528],\n",
       "  [16, 7554, 21827, 749, 9857, 2646],\n",
       "  [16, 40, 2497, 326, 428, 3807],\n",
       "  [16, 6610, 16304, 0, 4162, 319],\n",
       "  [15, 50, 707, 340, 379, 262],\n",
       "  [16, 40, 6198, 2497, 428, 319],\n",
       "  [16, 43977, 25, 383, 4403, 13069],\n",
       "  [16, 1135, 1094, 88, 9214, 789],\n",
       "  [16, 5779, 11, 484, 1908, 340],\n",
       "  [16, 10970, 406, 6158, 6006, 32297],\n",
       "  [16, 1212, 2646, 3033, 734, 286],\n",
       "  [15, 76, 40302, 14509, 1559, 318],\n",
       "  [15, 37, 48437, 338, 645, 343],\n",
       "  [16, 1026, 338, 4998, 326, 14549],\n",
       "  [16, 10374, 16057, 9737, 271, 392],\n",
       "  [16, 3666, 5212, 290, 314, 3332],\n",
       "  [15, 28951, 64, 318, 257, 845],\n",
       "  [15, 40, 2497, 428, 3807, 257],\n",
       "  [16, 49141, 29330, 318, 14702, 11],\n",
       "  [15, 37887, 618, 257, 5581, 3182],\n",
       "  [15, 38413, 17049, 1760, 13, 317],\n",
       "  [16, 40, 7342, 366, 37, 28789],\n",
       "  [15, 17947, 3978, 11255, 1441, 1363],\n",
       "  [15, 33481, 5289, 743, 407, 423],\n",
       "  [16, 1212, 318, 407, 257, 14604],\n",
       "  [15, 1212, 4471, 20718, 514, 284],\n",
       "  [16, 40, 1807, 428, 373, 257],\n",
       "  [16, 1212, 373, 281, 3499, 2050],\n",
       "  [15, 51, 13, 57, 13, 2947],\n",
       "  [15, 464, 7110, 340, 338, 407],\n",
       "  [16, 3633, 262, 19661, 287, 428],\n",
       "  [15, 32, 7818, 26337, 25, 8381],\n",
       "  [16, 1212, 3807, 4394, 5626, 39],\n",
       "  [16, 11380, 886, 286, 262, 1621],\n",
       "  [16, 21447, 2444, 379, 257, 8294],\n",
       "  [15, 1722, 262, 3670, 5644, 612],\n",
       "  [15, 32, 12362, 922, 3807, 0],\n",
       "  [15, 1212, 3807, 3190, 4966, 24590],\n",
       "  [15, 44960, 27633, 4244, 290, 262],\n",
       "  [16, 1, 44, 18526, 1, 318],\n",
       "  [15, 38, 385, 6656, 10844, 468],\n",
       "  [15, 40, 7342, 428, 938, 1755],\n",
       "  [16, 1722, 287, 1703, 417, 494],\n",
       "  [15, 505, 1110, 2130, 531, 8781],\n",
       "  [16, 1212, 318, 1194, 286, 616],\n",
       "  [15, 1, 34, 963, 6495, 1],\n",
       "  [16, 40, 1422, 470, 1100, 262],\n",
       "  [16, 40, 373, 12451, 366, 28524],\n",
       "  [15, 4366, 2365, 499, 2771, 3660]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch_data():\n",
    "    label = random.choices(range(2), k=128)\n",
    "    question = random.choices(dataset, k=128)\n",
    "    question = [i['question'] for i in question]\n",
    "\n",
    "    question = [[tokenizer.convert_tokens_to_ids(str(l))] + q\n",
    "                for l, q in zip(label, question)]\n",
    "\n",
    "    return label, question\n",
    "\n",
    "\n",
    "get_batch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ee37d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pt2/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at model/lvwerra/gpt2-imdb were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at model/lvwerra/gpt2-imdb were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "model_ppo = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    'model/lvwerra/gpt2-imdb').to(device)\n",
    "model_ppo_ref = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    'model/lvwerra/gpt2-imdb').to(device)\n",
    "\n",
    "for i in model_ppo_ref.parameters():\n",
    "    i.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805234d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer_cls = AutoTokenizer.from_pretrained(\n",
    "    'tokenizer/lvwerra/distilbert-imdb')\n",
    "model_cls = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'model/lvwerra/distilbert-imdb').to(device)\n",
    "\n",
    "for i in model_cls.parameters():\n",
    "    i.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3e7a327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "         1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "         1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "         0, 1, 1, 0, 0, 1, 0, 0], device='cuda:0'),\n",
       " [tensor([  16,   40, 1053, 6810,  625,  262], device='cuda:0'),\n",
       "  tensor([  16, 1212, 3807,  373,   11,  286], device='cuda:0'),\n",
       "  tensor([  15, 3673,  881,  284,  910,  319], device='cuda:0'),\n",
       "  tensor([   15, 20459,  8066,   307,  1327,   284], device='cuda:0'),\n",
       "  tensor([   15, 38743, 31140,  2492,   470,   262], device='cuda:0'),\n",
       "  tensor([  15, 2061,  257, 2085,  286,  257], device='cuda:0'),\n",
       "  tensor([   16,    40,   460,   766,  1521, 43442], device='cuda:0'),\n",
       "  tensor([  16,   40, 3521,  470, 4043,  284], device='cuda:0'),\n",
       "  tensor([16, 49, 13, 40, 13, 34], device='cuda:0'),\n",
       "  tensor([  15, 1212, 2646,  318,  257, 5003], device='cuda:0')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_question():\n",
    "    label, question = get_batch_data()\n",
    "    label = torch.LongTensor(label).to(device)\n",
    "\n",
    "    question = [torch.LongTensor(i).to(device) for i in question]\n",
    "\n",
    "    return label, question\n",
    "\n",
    "\n",
    "label, question = get_question()\n",
    "\n",
    "label, question[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b77e7d96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  812,   326,  3427, 14479,  3371,  6918,    11,  2592,   739,   262,\n",
       "         34731,   286,   366,  7353,    12, 23922,  1042,     1,   389,   783,\n",
       "          5371,   286, 11560,   257, 43122,   290,   326, 22041,  2331,  1234,\n",
       "           379,  9033], device='cuda:0'),\n",
       " tensor([ 1781,    11,  2208, 45002,    13,   314,  1392,   604,    14,   940,\n",
       "           737, 33443, 10544,    13,   383,  7205,   373,   407,  2089,    11,\n",
       "          4632,   616,  6000, 18641,   286,   262,  3807,  2277,   616,  1767,\n",
       "            13,   406], device='cuda:0'),\n",
       " tensor([  287,   597,   584,  2912,   780,  1111,   423,   617,   922,  2173,\n",
       "           329,   514,    13,  1119,  1111,  4893,   845,   880,   703,  6283,\n",
       "           262,  1621,   318,   290,   635, 14846, 14397,   319,   257,  1241,\n",
       "           326,   338], device='cuda:0'),\n",
       " tensor([  804,   379,   606,   290,   852, 15049,   644,   257,  2823,   428,\n",
       "           318,    13,   314,   460,   470,   787,   503,   257,  2060,  1517,\n",
       "           546,   262,  3807,   314,   892,   475,   314,   481,  9159,  1312,\n",
       "          8288,   428], device='cuda:0'),\n",
       " tensor([  691,  6575,  8442,   286,   262,   705,  2154,    82,   508,   336,\n",
       "          2954,   357,   392, 20143,   284,  4656,   737, 50256],\n",
       "        device='cuda:0'),\n",
       " tensor([ 3807,   329,  1105,   290,  1315,   614, 44979, 29847,  1671,  1220,\n",
       "          6927,  1671, 11037,  1212,   530,   318,   257, 36364,  5287,    13,\n",
       "          3887,  2168,  1281,   994,   468,  4054,    11,  4556,   286,  1781,\n",
       "           345,   389], device='cuda:0'),\n",
       " tensor([  290, 27583,  1661,  3947,  1342,  3616,   913,   777,  1528,    13,\n",
       "           921,   892,  1353, 29838, 10544,   460,  3491,   287,   257,  4101,\n",
       "          5664,  3807,   290,  1577,  9739,   477,   832,   262,   835,    30,\n",
       "          1892,   287], device='cuda:0'),\n",
       " tensor([ 651,  736,  319,  262, 6614,   13, 1675,  307, 3148,   11,  340,  373,\n",
       "         5867, 3625,  890,   13,  843,  340,  373,  546, 9796, 3625,  416, 3439,\n",
       "         3625,  890,   13, 1320, 1838,  340,  804,  588], device='cuda:0'),\n",
       " tensor([   13,    48,    13,   379,   262,  7369,   838, 19245,   319, 42490,\n",
       "           318,   407,   284,   307,   287,   262,  1551,  1643, 14702,    13,\n",
       "          1026,  2125,   470,  1327,   284,  1560,   655,   703, 10927,  3619,\n",
       "         40365,   318], device='cuda:0'),\n",
       " tensor([ 9875,   422,   923,   284,  5461,   351,   257, 21840,  4226,   290,\n",
       "           281,  6275,  3350,   326, 16316, 23501, 22605,   351,   257, 23754,\n",
       "          2854,   355, 34705,  2241,    13, 30436,  6550, 30673, 11392,   318,\n",
       "           257,  3756], device='cuda:0')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#包装类,用于生成\n",
    "def generate(input_ids):\n",
    "    return model_ppo.generate(input_ids=input_ids,\n",
    "                              min_length=-1,\n",
    "                              top_k=0.0,\n",
    "                              top_p=1.0,\n",
    "                              do_sample=True,\n",
    "                              pad_token_id=tokenizer.pad_token_id,\n",
    "                              max_new_tokens=32,\n",
    "                              eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "def get_answer(question):\n",
    "    #如果question的长度确定,这里可以转换成批运算\n",
    "    if True:\n",
    "        answer = generate(torch.stack(question))\n",
    "\n",
    "        answer_new = []\n",
    "        for i in answer:\n",
    "            if tokenizer.eos_token_id not in i:\n",
    "                answer_new.append(i.unsqueeze(0))\n",
    "                continue\n",
    "            split = i.tolist().index(tokenizer.eos_token_id) + 1\n",
    "            answer_new.append(i[:split].unsqueeze(0))\n",
    "        answer = answer_new\n",
    "    else:\n",
    "        answer = [generate(i.unsqueeze(0)) for i in question]\n",
    "\n",
    "    #裁剪,只要生成的部分\n",
    "    answer = [a[0, len(q):] for q, a in zip(question, answer)]\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "answer = get_answer(question)\n",
    "\n",
    "answer[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "720b4c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0790, -2.7419,  0.0679, -1.6332,  0.6677,  2.6076,  0.1573, -1.1461,\n",
       "         0.9582, -2.5122,  2.7525, -2.3757,  1.0361,  2.5690,  1.9334, -2.1080,\n",
       "        -1.7346,  0.6678, -1.2840, -0.0737,  1.8030, -1.5229, -1.9619, -1.7785,\n",
       "        -1.3398,  2.2020,  0.3973, -1.9178, -1.7657, -0.1403, -1.7730,  2.3305,\n",
       "         1.4456, -1.9255, -2.2839,  1.7811, -2.6034,  1.0441, -0.5461,  1.6941,\n",
       "         2.0946, -1.7392,  0.8828, -0.9679,  0.2184, -0.4701, -2.4185, -0.0232,\n",
       "        -0.0851,  2.2899, -2.2363, -2.2239, -1.2585,  1.6426, -0.6541,  0.9921,\n",
       "         0.2738,  0.2257,  1.1809, -1.2928,  2.2934,  2.8806,  1.1941, -1.3584,\n",
       "        -0.4178,  2.0240,  2.0432,  0.2561, -1.0444, -0.7836, -1.6604,  0.9020,\n",
       "        -2.4616,  0.5171,  0.3906,  1.7344,  1.1841, -1.4855,  2.4969, -1.4334,\n",
       "        -1.9962,  0.0924, -1.0023,  2.7003, -0.4900, -1.1669, -1.4747, -1.2043,\n",
       "         0.4568,  2.5980,  1.8720, -1.3922,  2.7693, -2.4912, -1.5629, -1.2905,\n",
       "         2.4059,  1.9968, -0.8069, -1.0914,  0.9309, -2.6868, -0.8958,  2.4808,\n",
       "        -2.9966, -1.4021, -2.5262,  1.7169, -1.8984, -0.9218,  2.3929,  2.2298,\n",
       "         0.2727,  0.1386, -0.5772,  0.9471,  0.2255,  1.2544,  1.9891,  1.8280,\n",
       "        -0.1826, -1.3578, -0.3683, -1.0374,  0.7353,  1.8427, -2.1924, -1.1235],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_reward(question, answer, label):\n",
    "    token = [q.tolist()[1:] + a.tolist() for q, a in zip(question, answer)]\n",
    "    token = [tokenizer.decode(i) for i in token]\n",
    "\n",
    "    token = tokenizer_cls(token,\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=512,\n",
    "                          return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_cls(**token).logits\n",
    "\n",
    "    return logits.gather(1, label.reshape(-1, 1)).squeeze(1)\n",
    "\n",
    "\n",
    "reward = get_reward(question, answer, label)\n",
    "\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ff781ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trl.trainer.ppo_trainer.PPOTrainer at 0x7f1e198a7b50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import PPOConfig, PPOTrainer\n",
    "\n",
    "config = PPOConfig(learning_rate=1e-5, batch_size=128)\n",
    "\n",
    "trainer = PPOTrainer(config,\n",
    "                     model_ppo,\n",
    "                     model_ppo_ref,\n",
    "                     tokenizer,\n",
    "                     dataset=dataset)\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1c70e8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.09299226105213165\n",
      "1First of all, this ->  movie is hardly stupid. However, it is one of the worst. It has two very real scenarios.Along with the efforts of a young couple of lost hero -2.487795829772949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 -0.023181095719337463\n",
      "0In my book \"Basic ->  Instinct\" released 17 years ago I read the book and think that such kind of things are the way the world is now. And quotes like \"All that -1.2708756923675537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.31382036209106445\n",
      "0Considering it's basically low ->  budget and a lot of black nudity and an odd response from the audience, the film is a pretty per ml resolution to an issue worthy of not being all that 1.1325664520263672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0.309106707572937\n",
      "0True stories make the best ->  work of good actors none that are born into their mid-seventies.<|endoftext|> -2.210801601409912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 1.0727498531341553\n",
      "1Cheesy 80's horror -> ...... composer: so beautifully chosen, I can say, has a on-screen performance even better.<|endoftext|> 2.0839757919311523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 1.4121887683868408\n",
      "0GBS wrote his own ->  script. One name near offense but atmospheric exposition, oddly still bland. Wipeout, rubbish. Violence, i slows.<|endoftext|> 2.3595705032348633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 1.7078416347503662\n",
      "0I was never in the ->  film.<|endoftext|> 0.9322419166564941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 2.1755259037017822\n",
      "1I must say, when ->  watching this movie, it is amazing.<|endoftext|> 2.4521334171295166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 2.1622257232666016\n",
      "1This is one of the ->  best films of all time.<|endoftext|> 2.7308685779571533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 2.1979665756225586\n",
      "1I resisted watching 15 Park ->  but so loved seeing it!<|endoftext|> 2.1706929206848145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 2.2108092308044434\n",
      "0This is a Laurel & ->  Hardy terrible movie.<|endoftext|> 1.9358655214309692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 2.288398504257202\n",
      "1This was the first Mickey ->  Renn masteringcom to medieval master and movie makers, and truly supported their development toward English film. highly recommended.<|endoftext|> 2.7567362785339355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 2.1484575271606445\n",
      "1There have been several films ->  and tv. This is great film for the ladies.<|endoftext|> 2.491626739501953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 2.2387020587921143\n",
      "1K Murli Mohan ->  in happiness that was tremendously 1934, and terrific movie.<|endoftext|> 2.5319623947143555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 2.111945629119873\n",
      "0I don't really know ->  how one will figure out how he's going to play the story, but this is a bad movie!<|endoftext|> 2.4522318840026855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 2.228043556213379\n",
      "1I will never get back ->  this last glorious film.<|endoftext|> 2.100973606109619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 2.0983808040618896\n",
      "1Night of the Twisters -> . Truly one of the best I've seen.<|endoftext|> 2.794154644012451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 2.1417503356933594\n",
      "1This film was a new ->  masterpiece.<|endoftext|> 2.473341464996338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 2.2145256996154785\n",
      "1William Faulkner was ->  just perfect! An excellent movie.<|endoftext|> 2.788565158843994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 2.0492758750915527\n",
      "0Mighty Morphin Power ->  is an embarrassment.<|endoftext|> 2.2804903984069824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for epoch in range(200):\n",
    "    label, question = get_question()\n",
    "    answer = get_answer(question)\n",
    "    reward = get_reward(question, answer, label)\n",
    "\n",
    "    trainer.step(question, answer, [i for i in reward])\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch, reward.mean().item())\n",
    "\n",
    "        question = tokenizer.decode(question[0].tolist())\n",
    "        answer = tokenizer.decode(answer[0].tolist())\n",
    "        reward = reward[0].item()\n",
    "\n",
    "        #0差评,1好评\n",
    "        print(question, '->', answer, reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt2]",
   "language": "python",
   "name": "conda-env-pt2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
